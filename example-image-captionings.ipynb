{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceType": "datasetVersion",
     "sourceId": 12231838,
     "datasetId": 7706960,
     "databundleVersionId": 12776820
    },
    {
     "sourceType": "datasetVersion",
     "sourceId": 2271054,
     "datasetId": 76785,
     "databundleVersionId": 2312124
    }
   ],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import sys\nimport importlib.util\n\nfile_path = '/kaggle/input/classok/classok.py'\nmodule_name = 'classok'\n\nspec = importlib.util.spec_from_file_location(module_name, file_path)\nclassok = importlib.util.module_from_spec(spec)\nsys.modules[module_name] = classok\nspec.loader.exec_module(classok)",
   "metadata": {
    "id": "lxsCo0-tZDiB",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-06-20T20:25:35.557579Z",
     "iopub.execute_input": "2025-06-20T20:25:35.557880Z",
     "iopub.status.idle": "2025-06-20T20:26:13.013853Z",
     "shell.execute_reply.started": "2025-06-20T20:25:35.557854Z",
     "shell.execute_reply": "2025-06-20T20:26:13.013278Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-06-20 20:25:57.314016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750451157.542096      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750451157.606485      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "from classok import ImageCaptioner",
   "metadata": {
    "id": "Ewe9gmQkZMEV",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-06-20T20:26:13.014476Z",
     "iopub.execute_input": "2025-06-20T20:26:13.014929Z",
     "iopub.status.idle": "2025-06-20T20:26:13.018537Z",
     "shell.execute_reply.started": "2025-06-20T20:26:13.014898Z",
     "shell.execute_reply": "2025-06-20T20:26:13.017814Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "# IdeficsForVisionText2Text modellt haszn\u00e1lva",
   "metadata": {
    "id": "xxTBQcVxXf7j"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoProcessor, IdeficsForVisionText2Text\nimport torch\nimport json\n\nmodel_id = \"HuggingFaceM4/idefics-9b-instruct\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16\n\nprocessor = AutoProcessor.from_pretrained(model_id)\nmodel = IdeficsForVisionText2Text.from_pretrained(\n    model_id,\n    torch_dtype=dtype,\n    device_map=\"auto\",\n    offload_buffers=True\n)\n\n# --- Load label map ---\nwith open(\"/kaggle/input/pytorch-challange-flower-dataset/cat_to_name.json\", \"r\") as f:\n    label_map = json.load(f)\n\n# --- Prompt ---\nprompt_template = (\n\"\"\"User: Provide a detailed description of the flower in the image:\n\n    Example: \"The poinsettia has large, vividly colored bracts, often mistaken for petals. The bracts form a star-shaped rosette around small yellow flowers in the center. The bracts come in shades of red, white, pink, orange, or variegated blends, creating a strong contrast with dark green leaves. Leaf texture is smooth or lightly veined, with pointed or rounded tips depending on the cultivar. The flowers are yellow-green and surrounded by tiny red glands that secrete nectar.\"\n\n    1. What is the overall shape of the flower's bloom? Is it symmetrical or asymmetrical? Describe the petals' shape.\n    2. What color are the petals? Are there any gradients or patterns? How does the color change in different light?\n    3. Describe the texture of the petals. Are they smooth, velvety, or waxy? Do they reflect light, appearing glossy or matte?\n    4. What does the center of the flower look like? Are there visible stamens or pistils? What colors and textures are visible?\n    5. What is the size of the flower's bloom? Is it compact or spread out?\n    6. How are the petals arranged? Are they spaced evenly, overlapping, or layered? Are there any visible veins or markings?\n    7. Are leaves visible? Describe their shape, color, and texture.\n    8. What is the condition of the flower? Is it fully open, partially open, or wilting?\n    9. Is the flower growing alone or in a group? How are the flowers arranged? Are they at different blooming stages?\n    10. Where is the flower found? Is it in a garden, field, or forest? Describe the surroundings.\n    11. How does the flower appear under different lighting? Does it have shadows, translucence, or reflections?\n    12. Is the flower in full bloom or at a particular stage? How does it compare to other flowers in different blooming stages?\n    13. What visual features distinguish this flower, such as shape, color, or texture?\n    14. How does the environment affect the flower\u2019s appearance? Is it in rocky, moist, or dry soil?\n\n    Assistant:\n\n    \"\"\"\n)\n\ncaptioner = ImageCaptioner(\n    model=model,\n    processor=processor,\n    base_folder=\"/kaggle/input/pytorch-challange-flower-dataset/dataset/train\",\n    label_map=label_map,\n    output_json_path=\"/kaggle/working/idefics_flower_captions.json\",\n    prompt=prompt_template,\n    model_type=\"idefics\",\n    device=device,\n    dtype=dtype\n)\n\n\ncustom_image_paths = [\n    \"/kaggle/input/pytorch-challange-flower-dataset/dataset/train/1/image_06734.jpg\"]\n\ncaptioner.caption_selected_images(custom_image_paths)\n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493,
     "referenced_widgets": [
      "13828fb26a674528b2b9ff4e6171a9bf",
      "6462ee2b246d4d0fbf78324ee9a4499e",
      "1bc23bf557fc44a9af35e68b564f3cf5",
      "46432af6fc8c42199f3851a531ccb23c",
      "46dcca7b2dc14f169d2e59ff84df0d75",
      "22a504a58e554c488bf8d171127558ec",
      "5f032106e5f4418cb2d9e80901d7b02d",
      "1e1da7aaecc44c3fa196a0903cc97a30",
      "b24941dd8efb4ecb9b6d8887db72c654",
      "7601974dfbbe473dab27f391f7774fd2",
      "7c06b3f87132418eb35784b5a15e2afb",
      "d67b0ff1e64544c2987eee948204aeda",
      "d79534b629a24307bc8afcdcba1f09bb",
      "a3eeaa8fb1b34ec785adffb0dc59599a",
      "5aff5d0054f043debcd66007fb6aff0e",
      "d0d7f91f02054e4fb129541f92733912",
      "412c681131004258a27037fcdeeee29d",
      "c2d1022438104856803cfe822be39f01",
      "4b71088b5b9c438c8b9d5d11616876af",
      "33f2fcd250cd4d42a54b98b778fb6939",
      "bc1e3b5af0b8466fb8cc945cb8d4876b",
      "910270a755ea4aa4a8088d7e5c635b42",
      "bbb88fb5bf0a43b6b2c0da0813eb2334",
      "c29e0bbc76cc4901b7bee9aa6e239b7b",
      "28bbccf175eb404eaa1af6475590abde",
      "003592c790b84655be87993d977cb127",
      "8d41cf730ce04a669afe3342c774a4cd",
      "4646c02726124079b31a1b7d1407909b",
      "b5dedc73800546be9fa616843da27757",
      "f27e175fa78043ae8435286410cf5700",
      "21154a806ac240b8a2ec57f5872a995c",
      "4334ca7f46414e468855587478301a5d",
      "35e4b68d606745ab84d107946859025a",
      "8ceb626f1bc14136b373a864704c57aa",
      "78562b606f4043f0ba0628a2fcebbc34",
      "26c8f6011d634536b83a6808d24c2a20",
      "87c2dfebb4824c6ea21a56dc76674ee9",
      "0402b838a4934c658f29545f8f39d49f",
      "3104eff6ef2e4cf9a74e02b751c3d8aa",
      "2ce8b30a200f40bdb4458e6537db85af",
      "ee9b2bae4585400dbcb1d848dc6b9260",
      "f4e63cddf077445eb523a2684976c615",
      "1826a340d8004c0e94cc258986475d3c",
      "83e64b415d6046179601572f7e0655eb",
      "241d82ce844440a080bafc0d2233f673",
      "a4c27bf2611e4afdbd64be3c76b3abf9",
      "2f34fe868a1141f192c5b20ed0ba9fb9",
      "584d208b802a4c7abfc7623ab41a8d88",
      "8f3ac757e07f4943ab02f63cf12c749c",
      "098be22d6ddc4ed28f85caa41eefcb0c",
      "b265353ba2ca4e35ae77660899680f14",
      "f175d9631505491097ccc9245447e3fb",
      "209095a989784475abb7fb597fcc9947",
      "76be40a1b3f04d0ea85fd1066356d0b1",
      "143d288f59e14c9b94ffc2b50d96ba96",
      "73274c5733f14f468f2fcdfd4e0fd34f",
      "c8dd7579ed7a43eb94bed2321c682326",
      "75b7f05da0114cdbb187f203ff2b8e64",
      "b8a5b32586bf492693c52a0b328b8228",
      "6757f0dedf0340b0b3f0cfc44a039de2",
      "59670d17d7b244ab9275d3f45778d046",
      "f5c9d3c31fd8460f8792eba8bfb9007c",
      "f89d361372f64d8eadbff349b7cea847",
      "1d3d34011d9a4bd68d4650c0db4f84b4",
      "2f79ddd313404467a2375c88126ec7b2",
      "6f0000750e084af68cdf3f7f4d390794",
      "bc1cb73b0af746a8b44aecd90d27b285",
      "cdc381ea8ed942c29960125473844462",
      "3a8898dfdfe5491b96452cdf4701cfad",
      "1f118f3387f14f5498486bcdb8ed1afd",
      "cb8ea91d087b411b9254d525cc8b6e75",
      "25179992faee4ce4abe50adf88fe53b2",
      "470e5aa0d9e34a56b9e514def01c5495",
      "469cf6f1a57644eaae2cf5ff92df034d",
      "3be0c56364244421814180adc56e4102",
      "a500f1d06ba742b6a1df6c5055af2d3f",
      "6d8adde21a0c47228569d98978be92b3",
      "72c93301accf428b92cd8eac4dcc0ec1",
      "82a3352ea9424d37bfd4706b3597cdbc",
      "5d6f986e7d6d4fd69235ccbd89ea0ad2",
      "b4a42afa827048dfb124f7d5709717e7",
      "d52bc1e74e09466db9828d1c81c45eeb",
      "1e43b29074e64076bb361e1e849537c7",
      "d494624b9e5f467dbe9b0ae626ab21e4",
      "d992fb81d663446582d6e58b7f06d613",
      "b5d04363610e42c2bed4e0749ef09ca3",
      "7b6a734b60a24128b768df74f72107bf",
      "94b42873899543619ddf093cf928a332",
      "111b267a32474e87a15fb91da0de5dbd",
      "3e4c8add11a6421c8da1511019afba53",
      "810994e497d44f23a77a5ddc5417abc8",
      "d9a814c3208247aba1add97b222e922a",
      "ee93bbde013e4be3826c902b831307c7",
      "c2cc166075d44019b3fe0f1ad2e35701",
      "055f1eb4baa44b14b84b63cb8e7117ca",
      "470b35cbcd96411b848fec900413a341",
      "0687c0a550e143a6a4032fef1ecbcadc",
      "2bcf471f49264ea391bc10f10488f4b1",
      "2c5884d75df84da69017d5ee41f38be3",
      "7f61fa48788448f09e4fc43cfc6c2ce3",
      "9d6a1a346e85484dbb01c3729d6bf1f1",
      "9de7d76abfcd41889f926a54e181fff0",
      "1a7da159349c48d79d4efa228126412d",
      "98da3ebc32e24bb6888f52e77fc7a954",
      "55ce96f5dd1a4208bbd0d46eeccf8b45",
      "5b8910cb0fd040d98a0f9b29e6e4b802",
      "7e789002e75c44fdaa8e4af5aac5ea01",
      "a0284eac078148509c9d2b3792f7a464",
      "331c6707caf647cf8f78a3348e43921b",
      "2fa5cb490d4247cb8ede22730597ed35",
      "b65f8c617c8244a3a7eeed9945efb632",
      "598f7c3be163494eb42d56ad26425767",
      "49d9845bdeaa4af5857aa0a6896929e3",
      "967775222abf49689f8b9582017e47fb",
      "cc8e27f6f0c54eaa8a33238d1630beae",
      "36af618b7b41411c9fafaf02189fa0dc",
      "0a72ac89b14a4736976907642108ca76",
      "3f943134484b4ce0acae5985dbee04a7",
      "550e73caf7124e1080b3ed92f2a43e56",
      "386559d32fd74cd2be165377a9b09caf",
      "afffd817347643c88d0ed1e4a008827e"
     ]
    },
    "id": "m_pcIYJqXVrR",
    "outputId": "2fe19e74-ddbb-4de5-e5c9-39123c231186",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-06-20T19:58:18.923553Z",
     "iopub.execute_input": "2025-06-20T19:58:18.924056Z",
     "iopub.status.idle": "2025-06-20T20:06:49.805980Z",
     "shell.execute_reply.started": "2025-06-20T19:58:18.924034Z",
     "shell.execute_reply": "2025-06-20T20:06:49.805263Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "136af8ceffe242e09927be65b99a3a04"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nCaptioning: /kaggle/input/pytorch-challange-flower-dataset/dataset/train/1/image_06734.jpg\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "# Blip2 modelt haszn\u00e1lva",
   "metadata": {
    "id": "GKv-jUUZXl1E"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import Blip2Processor, Blip2ForConditionalGeneration\nimport torch\nimport json\n\nmodel_id = \"Salesforce/blip2-flan-t5-xl\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16  # works best with BLIP-2 on GPU\n\n# Load processor and model for BLIP-2\nprocessor = Blip2Processor.from_pretrained(model_id)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=dtype,\n    device_map=\"auto\"\n)\n\n\n# --- Load label map ---\nwith open(\"/kaggle/input/pytorch-challange-flower-dataset/cat_to_name.json\", \"r\") as f:\n    label_map = json.load(f)\n\n# --- Prompt ---\nprompt_template = (\"\"\" 1. What is the overall shape of the flower's bloom? Is it symmetrical or asymmetrical? Describe the petals' shape.\n    2. What color are the petals? Are there any gradients or patterns? How does the color change in different light?\n    3. Describe the texture of the petals. Are they smooth, velvety, or waxy? Do they reflect light, appearing glossy or matte?\n    4. What does the center of the flower look like? Are there visible stamens or pistils? What colors and textures are visible?\n    5. What is the size of the flower's bloom? Is it compact or spread out?\n    6. How are the petals arranged? Are they spaced evenly, overlapping, or layered? Are there any visible veins or markings?\n    7. Are leaves visible? Describe their shape, color, and texture.\n    8. What is the condition of the flower? Is it fully open, partially open, or wilting?\n    9. Is the flower growing alone or in a group? How are the flowers arranged? Are they at different blooming stages?\n    10. Where is the flower found? Is it in a garden, field, or forest? Describe the surroundings.\n    11. How does the flower appear under different lighting? Does it have shadows, translucence, or reflections?\n    12. Is the flower in full bloom or at a particular stage? How does it compare to other flowers in different blooming stages?\n    13. What visual features distinguish this flower, such as shape, color, or texture?\n    14. How does the environment affect the flower\u2019s appearance? Is it in rocky, moist, or dry soil?\"\"\"\n)\n\n\n\n\ncaptioner = ImageCaptioner(\n    model=model,\n    processor=processor,\n    base_folder=\"/kaggle/input/pytorch-challange-flower-dataset/dataset/train\",\n    label_map=label_map,\n    output_json_path=\"/kaggle/working/blip2_flower_captions.json\",\n    prompt=prompt_template,\n    model_type=\"blip2\",\n    device=device,\n    dtype=dtype\n)\n\n\ncustom_image_paths = [\n    \"/kaggle/input/pytorch-challange-flower-dataset/dataset/train/1/image_06734.jpg\"]\n\ncaptioner.caption_selected_images(custom_image_paths)",
   "metadata": {
    "id": "RtLaTnhNXo-u",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-06-20T20:17:17.178483Z",
     "iopub.execute_input": "2025-06-20T20:17:17.179047Z",
     "iopub.status.idle": "2025-06-20T20:20:22.527629Z",
     "shell.execute_reply.started": "2025-06-20T20:17:17.178989Z",
     "shell.execute_reply": "2025-06-20T20:20:22.525906Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "600322753a2649f9909eaee3fa05dfc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/21.0k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bfc532c9f064907b57ded087090630a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "604606608e7b4e5786e6aa7b7b63fe15"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c712fc83d2594d3e9d78cdc470202d22"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9465106680954467906efa8f794c3343"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af72b92aa7554a6186516b2850d1c2ac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "437656b477be4375834c370f719c2fe5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd5c7fef10ce405cae25451124e7a79d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/128k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b95f5c2c89ad442c8f01a8ca4df77ec6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc6354b5e24f4c6cb0e9f06c45e80950"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/5.81G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93338f039d0a483199fb56fd9fe1b805"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/9.96G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "341bd4e69fd142d59d1a6817ccdd6f75"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a788629f7358467da05c23bcf0b8368d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a63606614e4417fbf2fad8c1e3bf12a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nCaptioning: /kaggle/input/pytorch-challange-flower-dataset/dataset/train/1/image_06734.jpg\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "# Llava modelt haszn\u00e1lva",
   "metadata": {
    "id": "PFgfNhuKXspp"
   }
  },
  {
   "cell_type": "code",
   "source": "from transformers import LlavaProcessor, LlavaForConditionalGeneration\nimport torch\nimport json\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16\n\nprocessor = LlavaProcessor.from_pretrained(model_id)\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id,\n    torch_dtype=dtype,\n    device_map=\"auto\"\n)\n\n\n\n\n# --- Load label map ---\nwith open(\"/kaggle/input/pytorch-challange-flower-dataset/cat_to_name.json\", \"r\") as f:\n    label_map = json.load(f)\n\n# --- Prompt ---\nprompt_template = (\n    \"\"\"<image>\\nUSER:  1. What is the overall shape of the flower's bloom? Is it symmetrical or asymmetrical? Describe the petals' shape.\n    2. What color are the petals? Are there any gradients or patterns? How does the color change in different light?\n    3. Describe the texture of the petals. Are they smooth, velvety, or waxy? Do they reflect light, appearing glossy or matte?\n    4. What does the center of the flower look like? Are there visible stamens or pistils? What colors and textures are visible?\n    5. What is the size of the flower's bloom? Is it compact or spread out?\n    6. How are the petals arranged? Are they spaced evenly, overlapping, or layered? Are there any visible veins or markings?\n    7. Are leaves visible? Describe their shape, color, and texture.\n    8. What is the condition of the flower? Is it fully open, partially open, or wilting?\n    9. Is the flower growing alone or in a group? How are the flowers arranged? Are they at different blooming stages?\n    10. Where is the flower found? Is it in a garden, field, or forest? Describe the surroundings.\n    11. How does the flower appear under different lighting? Does it have shadows, translucence, or reflections?\n    12. Is the flower in full bloom or at a particular stage? How does it compare to other flowers in different blooming stages?\n    13. What visual features distinguish this flower, such as shape, color, or texture?\n    14. How does the environment affect the flower\u2019s appearance? Is it in rocky, moist, or dry soil?\\nASSISTANT:\"\"\"\n)\n\n\n\ncaptioner = ImageCaptioner(\n    model=model,\n    processor=processor,\n    base_folder=\"/kaggle/input/pytorch-challange-flower-dataset/dataset/train\",\n    label_map=label_map,\n    output_json_path=\"/kaggle/working/llava_flower_captions.json\",\n    prompt=prompt_template,\n    model_type=\"llava\",\n    device=device,\n    dtype=dtype\n)\n\n\n\n\ncustom_image_paths = [\n    \"/kaggle/input/pytorch-challange-flower-dataset/dataset/train/1/image_06734.jpg\"]\n\ncaptioner.caption_selected_images(custom_image_paths)\n",
   "metadata": {
    "id": "QpTHDuU1Xvpd",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-06-20T20:26:13.019875Z",
     "iopub.execute_input": "2025-06-20T20:26:13.020056Z",
     "iopub.status.idle": "2025-06-20T20:28:15.199995Z",
     "shell.execute_reply.started": "2025-06-20T20:26:13.020042Z",
     "shell.execute_reply": "2025-06-20T20:28:15.199383Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05c6a74554a34b3a95ffc0e17080a621"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.45k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "064f25cfe60543e8af520842284e6642"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5e3581f6bdb48e8b37f70062189b1dc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/3.62M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2f59da334e144fda2b6a742a49959cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b24c7e2355847779ca95edae4eed57a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da051e332c6843cc9c26370881e9f164"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "chat_template.jinja:   0%|          | 0.00/674 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a03bf4fab1e4e91ae2ca3c012b9d448"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "processor_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "47cf5daeecc343fdb7068127b33630ff"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "chat_template.json:   0%|          | 0.00/701 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "092e5778677147a493437423033e0b2c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32a47bc4d1904f59907ba6b73f582d46"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "943076721692451888f629456f25043d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3b938eb42684fa08256ba22ac18bfb9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4bdb71e8e5fa4f6a8659bd5d2f467092"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b53f5c76e3641758cd53319eb782d02"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19c4deeb983649f79496dd2ff734c1b3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce2eded865c743d3bc531c5f44f8ce47"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f526e4d7e6284fc183de52a4d7c87b0e"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "\nCaptioning: /kaggle/input/pytorch-challange-flower-dataset/dataset/train/1/image_06734.jpg\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
